{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re,os,json\n",
    "from keras import layers,models,utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_everything():\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 250000\n",
    "EMBEDDING_SIZE = 100\n",
    "MAX_DOC_LEN = 128\n",
    "MIN_DOC_LEN = 12\n",
    "# xml_7z = utils.get_file(\n",
    "#     fname='travel.stackexchange.com.7z',\n",
    "#     origin='https://ia800107.us.archive.org/27/items/stackexchange/travel.stackexchange.com.7z',\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95000/100000"
     ]
    }
   ],
   "source": [
    "def extract_stackexchange(filename, limit=100000):\n",
    "    json_file = filename + 'limit=%s.json' % limit\n",
    "\n",
    "    rows = []\n",
    "#     f = os.popen('7z x -so \"%s\" Posts.xml' % filename) #解压后得到Posts.xml文件\n",
    "    with open(filename) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = str(line)\n",
    "            if not line.startswith('  <row'):\n",
    "                continue\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print('\\r%05d/%05d' % (i, limit), end='', flush=True)\n",
    "\n",
    "            parts = line[6:-5].split('\"')\n",
    "            record = {}\n",
    "            for i in range(0, len(parts), 2):\n",
    "                k = parts[i].replace('=', '').strip()\n",
    "                v = parts[i+1].strip()\n",
    "                record[k] = v\n",
    "            rows.append(record)\n",
    "\n",
    "            if len(rows) > limit:\n",
    "                break\n",
    "    \n",
    "    with open(json_file, 'w') as fout:\n",
    "        json.dump(rows, fout)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "rows = extract_stackexchange('data/Posts.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(rows)\n",
    "df = df.set_index('Id', drop=False)\n",
    "df['Title'] = df['Title'].fillna('').astype('str')\n",
    "df['Tags'] = df['Tags'].fillna('').astype('str')\n",
    "df['Body'] = df['Body'].fillna('').astype('str')\n",
    "df['Id'] = df['Id'].astype('int')\n",
    "df['PostTypeId'] = df['PostTypeId'].astype('int')\n",
    "df['ViewCount'] = df['ViewCount'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['ViewCount'] > 500]['Title']\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words = VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df['Body'] +' '+ df['Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute  word tf/idf value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = sum(tokenizer.word_counts.values())\n",
    "idf = {k:np.log(total_count/v) for k,v in tokenizer.word_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_tokens'] = tokenizer.texts_to_sequences(df['Title']) # token-> id\n",
    "df['body_tokens'] = tokenizer.texts_to_sequences(df['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据增强，生成样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def data_genator(batch_size, negative_sample = 1):\n",
    "    \"\"\"\n",
    "    data augment 用来产生样本对[(a,b,0),(a,b,1)]\n",
    "    \"\"\"\n",
    "    questions = df[df['PostTypeId'] == 1]\n",
    "    all_q_id = list(questions.index) # 所有问题的idx\n",
    "    batch_x_a = []\n",
    "    batch_x_b = []\n",
    "    batch_y = []\n",
    "    \n",
    "    def _add(x_a,x_b,y):\n",
    "        batch_x_a.append(x_a[:MAX_DOC_LEN])\n",
    "        batch_x_b.append(x_b[:MAX_DOC_LEN])\n",
    "        batch_y.append(y)\n",
    "        \n",
    "    while True:\n",
    "        questions = questions.sample(frac=1.0)\n",
    "        for i, q in questions.iterrows():\n",
    "            _add(q['title_tokens'], q['body_tokens'], 1)\n",
    "            negative_q = random.sample(all_q_id, negative_sample)\n",
    "            for nq_id in negative_q:\n",
    "                _add(q['title_tokens'], df.at[nq_id, 'body_tokens'], 0) # 负样本\n",
    "            if len(batch_y) >= batch_size:\n",
    "                yield({\n",
    "                    'title':pad_sequences(batch_x_a, maxlen=MAX_DOC_LEN), # 默认向前pad\n",
    "                    'body':pad_sequences(batch_x_b, maxlen=MAX_DOC_LEN)\n",
    "                }, np.array(batch_y))\n",
    "                \n",
    "                batch_x_a = []\n",
    "                batch_x_b = []\n",
    "                batch_y = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sum_model_v1(embedding_size=EMBEDDING_SIZE,vocab_size=VOCAB_SIZE):\n",
    "    \"\"\"\n",
    "    返回的为两个模型,1、分类使用 2、embedding\n",
    "    \"\"\"\n",
    "    title = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    body = layers.Input(shape=(None,), dtype='int32', name='body')\n",
    "\n",
    "    # This layer can only be used as the first layer in a model\n",
    "    embedding = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=embedding_size)\n",
    "    mask = layers.Masking(mask_value=0)\n",
    "    def _combine_sum(v):\n",
    "        return K.sum(v, axis=2)\n",
    "    sum_layer = layers.Lambda(_combine_sum)\n",
    "    title_sum = sum_layer(mask(embedding(title)))\n",
    "    body_sum = sum_layer(mask(embedding(body)))\n",
    "    sim = layers.dot([title_sum, body_sum], normalize=True, axes=1)\n",
    "    sim_model = models.Model(input=[title, body], outputs=[sim])\n",
    "    sim_model.compile(loss='mae', optimizer='rmsprop')\n",
    "    sim_model.summary()\n",
    "    \n",
    "    embedding_model = models.Model(input=[title], outputs=[title_sum])\n",
    "    \n",
    "    return sim_model,embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看模型的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    25000000    title[0][0]                      \n",
      "                                                                 body[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 100)    0           embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           masking_1[0][0]                  \n",
      "                                                                 masking_1[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           lambda_1[0][0]                   \n",
      "                                                                 lambda_1[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 25,000,000\n",
      "Trainable params: 25,000,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "sim_model,embedding_model = sum_model_v1()\n",
    "# plot_model(sim_model,'model.png') # 绘制出模型的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 58s 577ms/step - loss: 0.5036\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 60s 600ms/step - loss: 0.5009\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 63s 631ms/step - loss: 0.5006\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 58s 577ms/step - loss: 0.5024\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 62s 621ms/step - loss: 0.5008\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 65s 653ms/step - loss: 0.4978\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 58s 580ms/step - loss: 0.4947\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 60s 595ms/step - loss: 0.4940\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 62s 619ms/step - loss: 0.4947\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 59s 587ms/step - loss: 0.4963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb40cfc438>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_model.fit_generator(data_genator(batch_size=128), epochs=10, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096/4096 [==============================] - 1s 283us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48628981900401413"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(data_genator(batch_size=4096))\n",
    "sim_model.evaluate(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df[df['PostTypeId'] == 1]['Title'].reset_index(drop=True)\n",
    "question_tokens = pad_sequences(tokenizer.texts_to_sequences(questions))\n",
    "\n",
    "class EmbeddingWrapper(object):\n",
    "    def __init__(self, model):\n",
    "        self._r = questions\n",
    "        self._i = {i:s for (i,s) in enumerate(questions)}\n",
    "        self._w = model.predict({'title':question_tokens}, verbose=1, batch_size=1024)\n",
    "        self._model = model\n",
    "        self._norm = np.sqrt(np.sum(self._w **2 + le-5, axis=1))\n",
    "    def nearest(self, sentence, n=10):\n",
    "        x = tokenizer.texts_to_sequences([sentence])\n",
    "        if len(x[0]) < MIN_DOC_LEN:\n",
    "            x[0] += [0] * (MIN_DOC_LEN - len(x))\n",
    "        e = self._model.predict(np.asarray(x))[0]\n",
    "        norm_e = np.sqrt(np.dot(e, e))\n",
    "        dist = np.dot(self._w, e) / (norm_e * self._norm)\n",
    "\n",
    "        top_idx = np.argsort(dist)[-n:]\n",
    "        return pd.DataFrame.from_records([\n",
    "            {'question': self._r[i], 'dist': float(dist[i])}\n",
    "            for i in top_idx\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34829/34829 [==============================] - 1s 34us/step\n"
     ]
    }
   ],
   "source": [
    "lookup = EmbeddingWrapper(model=embedding_model)\n",
    "lookup.nearest('Python Postgres object relational model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted model # todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_model(embedding_size, vocab_size, embedding_weight = None,idf_weight=None):\n",
    "    title = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    body = layers.Input(shape=(None,), dtype='int32', name='body')\n",
    "    def make_embedding(name):\n",
    "        if embedding_weight is not None:\n",
    "            embedding = layers.Embedding(mask_zero=True, \n",
    "                                         input_dim=vocab_size, \n",
    "                                         output_dim=w2v_weights.shape[1], \n",
    "                                         weights=[w2v_weights],\n",
    "                                        trainable=False,name=f'{name}/embedding')\n",
    "            \n",
    "        else:\n",
    "            embedding = layers.Embedding(mask_zero=True,\n",
    "                                        input_dim=vocab_size,\n",
    "                                        output_dim=embedding_size,\n",
    "                                        name=f'{name}/embedding')\n",
    "            \n",
    "            \n",
    "        if idf_weight is not None:\n",
    "            idf = layers.Embedding(mask_zero=True,\n",
    "                                   input_dim=vocab_size, \n",
    "                                   output_dim=1,\n",
    "                                   weights=[idf_weight],\n",
    "                                   trainable=False,\n",
    "                                   name=f'{name}/idf')\n",
    "        else:\n",
    "            idf = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=1, name=f'{name}/idf')\n",
    "        return embedding, idf\n",
    "    embedding_a, idf_a = make_embedding('a')\n",
    "    embedding_b, idf_b = make_embedding('b') # 需要共享参数\n",
    "    \n",
    "    mask = layers.Masking(mask_value=0)\n",
    "    def _combine_and_sum(aegs):\n",
    "        embedding, idf = args\n",
    "        return K.sum(embedding * K.abs(idf), axis=1)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
